{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Data\n",
    "\n",
    "One of the most important aspects of designing any machine learning system is to __understand your data__. In this notebook, we're going to load and visualize some very basic datasets so that you understand some of the various tools available to you in Python for working with data. You should __always try to visualize your data__ before you use it in any type of algorithm. First we will look at a random dataset, and then we'll use `sklearn` to study the Iris dataset and other famous datasets.\n",
    "\n",
    "_Note: some code segments have TODO comments in them. These comments are optional exercises for you to modify the code in a useful way, however they are not meant to be restrictive. Feel free to modify the code in this notebook any way you like; it's a great way to practice your coding skills._\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "You should have your own Anaconda virtual environment with all of the necessary Python modules installed. You can check by trying to import them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One really useful feature to know with Python is the `help()` function. You can call `help()` on just about anything, including modules and functions. Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(np.random.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generating Random Data\n",
    "\n",
    "Let's begin by generating a random dataset of 2-D data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can set a random seed to get \"reproducible\" randomness\n",
    "np.random.seed(912)\n",
    "\n",
    "# define a function to generate random data\n",
    "def random_data(num_samples):\n",
    "    \"\"\"\n",
    "    Generates random data in two dimensions.\n",
    "    Returns a (num_samples, 2) numpy array.\n",
    "    \"\"\"\n",
    "    # define the parameters (mean, covariance) of a normal distribution\n",
    "    mean = [0.5, 0.5]\n",
    "    cov = [[1,0], [0,100]]\n",
    "\n",
    "    # generate num_samples of 2D data\n",
    "    X = np.random.multivariate_normal(mean, cov, size=num_samples)\n",
    "    return X "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what this data looks like. We'll start small and draw 50 samples from our function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_data = random_data(50)\n",
    "print(\"Data generated of shape: (%d, %d)\" % rand_data.shape)\n",
    "print(rand_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Now we have some data to play with.\n",
    "\n",
    "## Plotting Univariate Data\n",
    "\n",
    "Let's try to generate some more samples and then plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 1000 random samples\n",
    "X = random_data(1000)\n",
    "\n",
    "# generate indices for X\n",
    "idx = range(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most basic aspects of a dataset is its __dimensionality__. For example, if a dataset is 2-dimensional, it means that the dataset has two variables that are influencing the outcomes. These dimensions are also called the __features__ of a dataset. In this case, `X` is our dataset, and we generated an index list so that we can plot each dimension individually. Let's plot each dimension of `X` using `matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a 2x1 figure for plotting\n",
    "_, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5), sharey=True)\n",
    "\n",
    "# plot the first dimension of X on the left\n",
    "ax1.scatter(idx, X[:, 0])\n",
    "ax1.set_title(\"Dimension 1 of X\")\n",
    "ax1.set_xlabel(\"Index\")\n",
    "ax1.set_ylabel(\"Value\")\n",
    "\n",
    "# plot the second dimension of X on the right\n",
    "ax2.scatter(idx, X[:, 1], c='r')\n",
    "ax2.set_title(\"Dimension 2 of X\")\n",
    "ax2.set_xlabel(\"Index\")\n",
    "ax2.set_ylabel(\"Value\")\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see how different the two dimensions are. Both dimensions are centered at 0 (to be exact, they are centered at 0.5). However, the first dimension spreads out _very little_ while the second dimension spreads out _a lot_. We typically use these two properties to broadly describe data: the center of the data is the __mean__ and the spread is called the __variance__. Later on in the course we will see why the above result -- features in a dataset with different variance -- matters when designing a machine learning system.\n",
    "\n",
    "But for now, let's continue to explore this type of data. We'll put this plotting code into a single function so that we can repeat and experiment with this process easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to generate and plot a random dataset\n",
    "def gen_vis_data(num_samples, colors=['b','r']):\n",
    "    \"\"\"\n",
    "    Generates num_samples random data from a distribution and \n",
    "    plots each dimension in a separate plot.\n",
    "    \n",
    "    Args:\n",
    "    num_samples: Number of samples to be generated. \n",
    "    colors (optional): The colors for the different plots.\n",
    "    \"\"\"\n",
    "    # generate random dataset\n",
    "    X = random_data(num_samples)\n",
    "    idx = range(num_samples)\n",
    "\n",
    "    # initialize a 2x1 figure for plotting\n",
    "    _, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5), sharey=True)\n",
    "\n",
    "    # plot the first dimension of X on the left\n",
    "    ax1.scatter(idx, X[:, 0])\n",
    "    ax1.set_title(\"Dimension 1 of X\")\n",
    "    ax1.set_xlabel(\"Index\")\n",
    "    ax1.set_ylabel(\"Value\")\n",
    "\n",
    "    # plot the second dimension of X on the right\n",
    "    ax2.scatter(idx, X[:, 1], c='r')\n",
    "    ax2.set_title(\"Dimension 2 of X\")\n",
    "    ax2.set_xlabel(\"Index\")\n",
    "    ax2.set_ylabel(\"Value\")\n",
    "\n",
    "    # display the plot\n",
    "    plt.show()\n",
    "\n",
    "    # TODO: modify this function so that you can specify the mean and covariance of the random data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our function by generating datasets of different sizes. Whenever you write code that works with data, it's always a good idea to test it on small data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_vis_data(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_vis_data(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_vis_data(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how closely packed the samples become as the sample size increases. This is one of the main reasons why it's important to have a sufficiently large amount of data: the closer together certain samples are, the easier it is for machine learning algorithms to make sense of the entire dataset. For example, in some of the plots you can see some points lying outside the spread; these points are called _outliers_. As we increase the sample size, it becomes easier to distinguish the outliers from the rest of the data.\n",
    "\n",
    "## Plotting Multivariate Data\n",
    "\n",
    "So far we've visualized _univariate_ (1-dimensional) data, but now let's visualize the entire dataset. We're going to use another Python module called `seaborn`, which provides some fancier plotting functions on top of `matplotlib`. Specifically, we're going to use a function called `jointplot` to visualize _bivariate_ (2-dimensional) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a 2-D dataset\n",
    "X = random_data(10000)\n",
    "\n",
    "# create a pandas dataframe with the data and column names\n",
    "df = pd.DataFrame(X, columns=[\"x\", \"y\"])\n",
    "\n",
    "# show the dataframe\n",
    "print(df)\n",
    "\n",
    "# plot the dataset with seaborn\n",
    "sns.jointplot(x=\"x\", y=\"y\", data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks cool! We essentially just collapsed the two plots from before into one. The `jointplot` function creates a `JointGrid`, which is actually three plots in one: the scatter plot, and two histograms for each of the individual dimensions. We also introduced something called a `DataFrame`, which is a structure that allows us to completely represent a dataset by storing both data and row/column names. This way we can name the dimensions in our dataset to make them more meaningful.\n",
    "\n",
    "## The Iris Dataset\n",
    "\n",
    "Now that we've seen how to generate and visualize random data, let's move on to the famous Iris dataset. Widely considered to be the \"Hello World\" for datasets, the Iris dataset was created in the early 1900's by the British statistician and biologist Ronald Fisher. This dataset has four features, which correspond to different measurements of an Iris flower, and 150 samples, which correspond to individual flowers. The dataset also has __labels__ or __targets__ associated with the data; that is, each sample is labeled according to its species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Iris dataset\n",
    "iris = sklearn.datasets.load_iris()\n",
    "\n",
    "# show dataset shape\n",
    "print(\"Iris data: (%d, %d)\" % iris.data.shape)\n",
    "print(\"Iris targets: (%d,)\" % iris.target.shape)\n",
    "\n",
    "# show feature names\n",
    "print(\"Feature names: \", iris.feature_names)\n",
    "\n",
    "# show label names\n",
    "print(\"Label names: \", iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the \"shape\" of the data denotes the number of samples and features, respectively, in the data. We can think of this dataset as an Excel spreadsheet with 150 rows and 4 columns with values, with an extra column for the labels. Even the smallest machine learning systems typically use datasets with thousands of samples (if not more) and any number of features depending on the application. Using the tools we've developed so far, we can at least visualize any 2 dimensions of a dataset.\n",
    "\n",
    "Now, let's load just a slice of the dataset and see what it looks like. As before we'll write code for a specific example and then we'll put it into a function so that we can repeat it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Iris dataset\n",
    "iris = sklearn.datasets.load_iris()\n",
    "\n",
    "# extract the first two dimensions\n",
    "iris_reduced = iris.data[:, 0:2]\n",
    "\n",
    "# plot first two dimensions\n",
    "plt.scatter(iris_reduced[:, 0], iris_reduced[:, 1], c=\"y\")\n",
    "plt.xlabel(iris.feature_names[0])\n",
    "plt.ylabel(iris.feature_names[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the distribution of this data is much more... interesting... than that of the random data from before. In reality, most real-world data is much more complex, which makes plots like these all the more important for understanding our data and using the right machine learning algorithms. This plot, for example, shows us that there seems to be two clusters of data above and below, and that we could probably draw a straight line to separate the two clusters. A dataset with this property is called _linearly separable_ since it can be cleanly separated into groups by one or more straight lines.\n",
    "\n",
    "As an excercise, plot the 1st and 3rd dimension using the code above. The third dimension is simply `iris.data[:, 2]`. Write your code in the cell below and then hit `Shift+Enter` to see the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sklearn.datasets.load_iris()\n",
    "\n",
    "# TODO: plot the 1st and 3rd dimensions of the Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's write a function that will allow us to plot any two dimensions against each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_iris_2d(iris, columns=[0, 1]):\n",
    "    \"\"\"\n",
    "    Plots the Iris dataset with given list of column indices.\n",
    "    Only the first two column indices are used.\n",
    "    \n",
    "    Args:\n",
    "    iris: Iris dataset loaded from sklearn\n",
    "    columns: The column indices. By default it'll plot the first and second dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    # extract x and y axes\n",
    "    x, y = iris.data[:, columns[0]], iris.data[:, columns[1]]\n",
    "\n",
    "    # plot x and y\n",
    "    plt.scatter(x, y, c=iris.target)\n",
    "    plt.xlabel(iris.feature_names[columns[0]])\n",
    "    plt.ylabel(iris.feature_names[columns[1]])\n",
    "    plt.show()\n",
    "\n",
    "    # TODO: add a legend to the scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iris_2d(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iris_2d(iris, [0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iris_2d(iris, [0, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iris_2d(iris, [1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa... why are there colors all of a sudden? If you look at our plotting function you'll see that we tweaked the call to `plt.scatter()` so that it would color samples by their label. That is, instead of providing a single color code like `\"y\"`, we can provide a list of color codes or even just numbers, and `matplotlib` will color each sample accordingly. In this case we provided the labels array so `matplotlib` simply picked three colors for each flower species.\n",
    "\n",
    "Adding color also shows us that the data isn't quite as linearly separable as we thought. In particular, while one of the species is separated pretty well, the other two species are consistently mangled together. What does this mean for us when designing a machine learning system for this data? It means that for whatever task we try to do with this data, such as classification or clustering, any machine learning algorithm we use will probably have more difficulty telling these two species apart.\n",
    "\n",
    "It looks like there are a lot of \"angles\" from which we can view the Iris dataset, but no single plot provides a complete view. However, `seaborn` has another supercharged plotting feature that is perfect for this kind of dataset, called `PairGrid`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Iris dataset (from seaborn)\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "\n",
    "# plot PairGrid showing 1-D histograms and 2-D scatter plots\n",
    "g = sns.PairGrid(iris, hue=\"species\")\n",
    "g = g.map_diag(plt.hist)\n",
    "g = g.map_offdiag(plt.scatter)\n",
    "g = g.add_legend()\n",
    "\n",
    "# TODO: change the lower half of the PairGrid to KDE plots (see sns.kdeplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, `PairGrid` is _really_ powerful because it essentially allows us to see a lot of angles all at once. It's also very configurable; we can \"map\" the diagonal, upper, and lower halves each to a separate plotting function, and in this case we can color the data by label and add a legend with little effort. The only downside is that `PairGrid` does not scale well to large datasets; as you increase the number of features (the Iris dataset only has four), the number of plots increases rapidly. You can still use `PairGrid` with a large dataset by selecting only a few features from the dataset.\n",
    "\n",
    "To learn more about what all seaborn can do, check out the [examples gallery](http://seaborn.pydata.org/examples/) and the API docs on the seaborn website.\n",
    "\n",
    "Let's look again at this issue of separability. It's clear that these two species (which we can identify as `versicolor` and `virginica` thanks to the legend) are stuck together, and any machine learning algorithm we throw at the data will have a hard time distinguishing these two. How can we deal with this problem? The primary way to make data easier to separate is to __add more features__; in other words, add a third or fourth axis along which we might be able to separate the two classes. Now we can't go back to 1936 and tell Ronald Fisher to add more types of measurements, but remember that __we're only looking at two dimensions at a time__. For any given scatter plot above there are two dimensions being excluded, which means that `versicolor` and `virginica` may in fact be separable if we just use the entire dataset instead of only two dimensions. We will revisit this question later when we begin to look at machine learning algorithms.\n",
    "\n",
    "## Assignment: Visualize Another Dataset\n",
    "\n",
    "The Iris dataset is one of the simplest datasets around, so it's really easy to understand from just a few visualizations. To practice your data-wrangling skills, pick one of the toy datasets from [scikit-learn](http://scikit-learn.org/stable/datasets/index.html#toy-datasets) or [seaborn](http://seaborn.pydata.org/generated/seaborn.load_dataset.html#seaborn.load_dataset) and create some visualizations that help you understand the data. Some good visualizations to get started include:\n",
    "- heatmaps\n",
    "- histograms / violinplots / kde plots\n",
    "- bar plots\n",
    "- pairwise scatter plots\n",
    "- regression plots\n",
    "\n",
    "`seaborn` provides all of these methods and more, and the [examples gallery](http://seaborn.pydata.org/examples/) is a great way to learn about them. Keep in mind that each type of visualization can be used in multiple ways, and some visualizations are more useful than others for a particular dataset.\n",
    "\n",
    "As you examine your dataset of choice, here are some basic questions to ask yourself:\n",
    "- How many __samples__ are in the dataset? How many __features__? How many __labels__?\n",
    "- Can you tell the difference between the features and the labels?\n",
    "- What is the __mean__ and __variance__ of each feature? More broadly, which features are \"spread out\" more?\n",
    "- Are any of the features __correlated__ with each other, or otherwise related in some interesting way?\n",
    "- Is the data __separable__? That is, is it easy to visually separate the data by labels?\n",
    "\n",
    "If you can visualize and understand one of these toy datasets, great! Try some of the other datasets! Every dataset is unique and will probably teach you something new about how to understand data through visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load and visualize another toy dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
