{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seizure Prediction with EEG Data\n",
    "\n",
    "Authors: Erin Shappell, Ben Shealy\n",
    "\n",
    "In this notebook we demonstrate how to use electroencephalogram (EEG) data to predict seizures in epileptic and neurotypical patients. This work contains the code used for Erin Shappell's Honors Thesis submitted in Spring 2020.\n",
    "\n",
    "In particular we draw from two EEG datasets:\n",
    "\n",
    "- [TUH EEG Corpus](https://www.isip.piconepress.com/projects/tuh_eeg/html/downloads.shtml)\n",
    "- [CHB-MIT Scalp EEG Database](https://archive.physionet.org/pn6/chbmit/)\n",
    "\n",
    "The raw data for this notebook can be obtained from these two websites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import sklearn.preprocessing\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Raw Data\n",
    "\n",
    "In this notebook we will be working primarily with the CHB-MIT dataset. The raw data can be downloaded directly from their website. We will use only the first five patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "DIRS=\"chb01 chb02 chb03 chb04 chb05\"\n",
    "\n",
    "for DIR in ${DIRS}; do\n",
    "    wget -r -A edf -np \"https://archive.physionet.org/pn6/chbmit/${DIR}\"\n",
    "done\n",
    "\n",
    "mv archive.physionet.org/pn6/chbmit .\n",
    "rm -rf archive.physionet.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "The CHB-MIT dataset contains data from many inviduals, but we will use only the first five. Each individual has approximately 40 recordings saved as EDF files, and each file has 23 channels associated with different areas of the brain. Additionally, each EDF file either contains or doesn't contain a single seizure event.\n",
    "\n",
    "We need to convert this data from EDF format to CSV format so that we can load it with pandas. Additionally, even with only the first five individuals there is still significantly more data than what we need for our prediction task, so we will extract a 50 second window from each EDF file. In particular we will extract the seizure event if it is present or the first 50 seconds if there is no seizure event. This way we will have plenty of positive and negative samples.\n",
    "\n",
    "__NOTE__: __You only need to run this code block once.__ The code will save a CSV file called `seizure.csv`, as well as individual CSV files for each channel. It is those CSV files that we will use for seizure prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data summary: \n",
    "# chb01: 46 files, seizures at 3, 4, 15, 16, 18, 21, 26\n",
    "# chb02: 35 files, seizures at 16, 19\n",
    "# chb03: 38 files, seizures at 1, 2, 3, 4, 34, 35, 36\n",
    "# chb04: 43 files, seizures at 5, 8, 28\n",
    "# chb05: 39 files, seizures at 6, 13, 16, 17, 22\n",
    "\n",
    "# sampling rate (in Hz)\n",
    "s_rate = 256 \n",
    "\n",
    "# Length (in seconds) of extraction window\n",
    "w_size = 50\n",
    "\n",
    "# indices of files that contain seizures\n",
    "s_loc = [\n",
    "    [3, 4, 15, 16, 18, 21, 26],\n",
    "    [16, 19],\n",
    "    [1, 2, 3, 4, 34, 35, 36],\n",
    "    [5, 8, 28],\n",
    "    [6, 13, 16, 17, 22]\n",
    "]\n",
    "\n",
    "# lower bounds of seizures\n",
    "s_begin = [\n",
    "    [2996, 1467, 1732, 1015, 1720, 327, 1862],\n",
    "    [130, 3369],\n",
    "    [362, 731, 432, 2162, 1982, 2592, 1725],\n",
    "    [7804, 6446, 1679],\n",
    "    [417, 1086, 2317, 2451, 2348]\n",
    "]\n",
    "\n",
    "# upper bounds of seizures (original, not used)\n",
    "# s_end = [\n",
    "#     [3036, 1494, 1772, 1066, 1810, 420, 1963],\n",
    "#     [212, 3378],\n",
    "#     [414, 796, 501, 2214, 2029, 2656, 1778],\n",
    "#     [7853, 6557, 1781],\n",
    "#     [532, 1196, 2413, 2571, 2465]\n",
    "# ]\n",
    "\n",
    "# initialize output dataframe\n",
    "df_extracted = pd.DataFrame()\n",
    "\n",
    "# process edf data from each patient directory\n",
    "s_i = 0\n",
    "\n",
    "for i in range(len(s_loc)):\n",
    "    # get directory path\n",
    "    file_path = 'chbmit/chb%02d' % (i + 1)\n",
    "    \n",
    "    # read all edf files in directory\n",
    "    edf_files = glob.glob(os.path.join(file_path, '*.edf'))\n",
    "    \n",
    "    for filename in edf_files:\n",
    "        print('Reading %s, s_i=%d' % (filename, s_i))\n",
    "\n",
    "        # read edf file\n",
    "        edf = mne.io.read_raw_edf(filename, verbose=False)\n",
    "\n",
    "        # create dataframe from edf data\n",
    "        df_raw = pd.DataFrame(data=edf.get_data().T, columns=edf.ch_names)\n",
    "\n",
    "        # extract index from filename\n",
    "        j = int(filename.replace('.', '_').split('_')[-2])\n",
    "\n",
    "        # extract seizure data if it is present\n",
    "        if j in s_loc[i]:\n",
    "            s_idx = s_loc[i].index(j)\n",
    "            begin = s_begin[i][s_idx]\n",
    "\n",
    "        # otherwise extract from the beginning\n",
    "        else:\n",
    "            begin = 0\n",
    "\n",
    "        # read each channel into dataframe\n",
    "        for channel in df_raw.columns:\n",
    "            # insert_data.reset_index(drop=True, inplace=True)\n",
    "            df_extracted['%s_%d' % (channel, s_i)] = df_raw.loc[begin * s_rate : (begin + w_size) * s_rate - 1, channel]\n",
    "\n",
    "        # rename weird column name\n",
    "        df_extracted.rename(columns={ '# FP1-F7_%d' % (s_i) : 'FP1-F7_%d' % (s_i) }, inplace=True)\n",
    "\n",
    "        # increment global index\n",
    "        s_i += 1\n",
    "\n",
    "# save extracted data to csv file\n",
    "df_extracted.to_csv('seizure.csv', index=False, float_format='%.8g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with only five individuals, our extracted dataset will be fairly large. Feel free to use more or fewer individuals, you just need to add the approriate information to the variables `s_loc` and `s_begin`. You can also experiment with the data extraction code: changing the window size, taking more time slices from each file at different starting points, etc.\n",
    "\n",
    "The `seizure.csv` file contains all of the data that we'll use for the rest of this notebook, but it will also be helpful to have separate CSV files for each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract channel names from extracted dataframe\n",
    "channels = [c.split('_')[0] for c in df_extracted.columns]\n",
    "channels = list(set(channels))\n",
    "\n",
    "# save separate csv files for each channel\n",
    "for channel in channels:\n",
    "    print(channel)\n",
    "\n",
    "    # extract columns that belong to channel\n",
    "    columns = [name for name in df_extracted.columns if name.startswith(channel)]\n",
    "\n",
    "    # create dataframe for channel\n",
    "    df_channel = df_extracted[columns]\n",
    "    df_channel.columns = [c.split('_')[1] for c in df_channel.columns]\n",
    "\n",
    "    # save dataframe to csv file\n",
    "    df_channel.to_csv('seizure.%s.csv' % (channel), index=False, float_format='%.8g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Seizure Classifier\n",
    "\n",
    "The first task that we will explore is using a convolutional neural network (CNN) to predict whether a time series sample represents a seizure event or not. CNNs are typically used for image classification, and they use 2D convolution because images have 2-dimensional structure. However, our time series is 1-dimensional, so we will use 1D convolution.\n",
    "\n",
    "Now you might think, isn't our time series data 2-dimensional since there are also channels? Yes, our data is 2-dimensional in the sense that each data sample is a 12,800 x 23 time series. However, the 23 channels in our data are independent of each other, so it would not make sense to convolve _across_ channels. Think of it in another way -- does it matter how the channels are ordered? By constrast, does it matter how the rows or columns in an image are ordered? That is the key difference here.\n",
    "\n",
    "First we define a few functions we will use to automate the train/test cycle for our CNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, train_size=0.8, n_timesteps=12800):\n",
    "    # compute split index\n",
    "    split_index = int(X.shape[0] * train_size)\n",
    "\n",
    "    # extract train set\n",
    "    X_train = X[:split_index, :n_timesteps]\n",
    "    y_train = y[:split_index]\n",
    "\n",
    "    # extract test set\n",
    "    X_test = X[split_index:, :n_timesteps]\n",
    "    y_test = y[split_index:]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def create_cnn(X_train, y_train, X_test, y_test):\n",
    "    # get data dimensions\n",
    "    n_timesteps = X_train.shape[1]\n",
    "    n_channels = X_train.shape[2]\n",
    "\n",
    "    # create model\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv1D(filters=64, kernel_size=3, padding='same', activation='relu', input_shape=(n_timesteps, n_channels)))\n",
    "    model.add(keras.layers.Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "    model.add(keras.layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(100, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def run_experiment():\n",
    "    # create train/test data\n",
    "    X_train, y_train, X_test, y_test = train_test_split(X, y)\n",
    "\n",
    "    # create model\n",
    "    model = create_cnn(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # print model summary\n",
    "    model.summary()\n",
    "\n",
    "    # train model\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1, verbose=True)\n",
    "\n",
    "    # evaluate model\n",
    "    score = model.evaluate(X_test, y_test)\n",
    "\n",
    "    # print test accuracy for test set\n",
    "    print('test accuracy: %f' % (score[1]))\n",
    "    \n",
    "    # plot the training accuracy\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('Training Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # plot the training loss\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Training Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must load the data and labels into a format that can be consumed by our CNN. For the data, we need to load our channel dataframes into a single 3D Numpy array with dimensions (n_samples x n_timesteps x n_channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: remove file 'seizure.ECG.csv' as it is not used and will cause an error\n",
    "# !rm -f seizure.ECG.csv\n",
    "\n",
    "# load csv files for each channel\n",
    "csv_files = glob.glob('seizure.*.csv')\n",
    "channels = []\n",
    "\n",
    "for filename in csv_files:\n",
    "    print(filename)\n",
    "\n",
    "    channel = pd.read_csv(filename)\n",
    "    channel = channel.values.T\n",
    "    channels.append(channel)\n",
    "\n",
    "# stack channels into 3-d numpy array\n",
    "X = np.dstack(channels)\n",
    "\n",
    "# print data dimensions (n_samples x n_timesteps x n_channels)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the labels, we have the list of indices of positive examples (seizures), we just need to transform this list into a single vector of labels for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices of files that contain seizures\n",
    "s_loc = [\n",
    "    [3, 4, 15, 16, 18, 21, 26],\n",
    "    [16, 19],\n",
    "    [1, 2, 3, 4, 34, 35, 36],\n",
    "    [5, 8, 28],\n",
    "    [6, 13, 16, 17, 22]\n",
    "]\n",
    "\n",
    "# compute labels from indices\n",
    "y = []\n",
    "\n",
    "for i in range(len(s_loc)):\n",
    "    # get directory path\n",
    "    file_path = 'chbmit/chb%02d' % (i + 1)\n",
    "\n",
    "    # read all edf files in directory\n",
    "    edf_files = glob.glob(os.path.join(file_path, '*.edf'))\n",
    "    \n",
    "    for filename in edf_files:\n",
    "        # extract index from filename\n",
    "        j = int(filename.replace('.', '_').split('_')[-2])\n",
    "\n",
    "        # label positive example (seizure)\n",
    "        if j in s_loc[i]:\n",
    "            y.append(1)\n",
    "\n",
    "        # label negative example (no seizure)\n",
    "        else:\n",
    "            y.append(0)\n",
    "\n",
    "# print label stats                \n",
    "print(\"Number of positive examples: %d\" % (y.count(1)))\n",
    "print(\"Number of negative examples: %d\" % (y.count(0)))\n",
    "\n",
    "# convert labels to numpy array\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train and evaluate our CNN. We've encapsulated all of the code into a single function to make it really simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Time Series Forecasting\n",
    "\n",
    "The second task we will explore is using a long short-term memory (LSTM) model to perform time-series forecasting with our dataset. In other words, given the beginning portion of a time series, can we predict what the next $n$ time steps will be? This kind of application could be very useful to predict a seizure _before_ it happens. Since we now have a CNN model that can predict with high accuracy whether a time series is a seizure event, all we have to do is feed the time series forecast from the LSTM into the CNN classifier. If the LSTM can accurately predict what the next time points will be, and the CNN can accurately predict whether those time points represent a seizure event, then we will be well on our way to predicting seizures before they happen!\n",
    "\n",
    "For now we will focus on _single-step_ forecasting, where we only predict one time point at a time. In practice we will need to predict many time steps at once in order to be fast enough for real-time applications.\n",
    "\n",
    "Sources used for reference:\n",
    "- https://towardsdatascience.com/using-lstms-to-forecast-time-series-4ab688386b1f\n",
    "- https://github.com/kmsravindra/ML-AI-experiments/blob/master/AI/LSTM-time_series/LSTM%20-%20Sine%20wave%20predictor.ipynb\n",
    "\n",
    "__NOTE__: The multi-step forecasting code (`n_steps > 1`) does not work yet. So that would be a good place to start if you're looking to work with this project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_create(window_size):\n",
    "    # create lstm model\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.LSTM(window_size, return_sequences=True, input_shape=(window_size, 1)))\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "    model.add(keras.layers.LSTM(256))\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    model.add(keras.layers.Activation(\"linear\"))\n",
    "\n",
    "    # compile model\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def lstm_train(model, X_train, y_train):\n",
    "    t1 = time.time()\n",
    "    model.fit(X_train, y_train, batch_size=512, epochs=3, validation_split=0.1, verbose=False)\n",
    "    t2 = time.time()\n",
    "\n",
    "    print('train time: %f s' % (t2 - t1))\n",
    "\n",
    "def lstm_predict(model, X_test, y_test, scaler, n_steps):\n",
    "    # compute single-step prediction\n",
    "    if n_steps == 1:\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "    # or compute multi-step prediction (note: does not work yet)\n",
    "    else:\n",
    "        y_pred = np.empty(n_steps)\n",
    "        X_window = X_test[0:1, :]\n",
    "\n",
    "        for i in range(n_steps):\n",
    "            # compute single-step prediction\n",
    "            y_pred_i = model.predict(X_window)\n",
    "            y_pred_i = y_pred_i.reshape(1, 1, 1)\n",
    "\n",
    "            # append single-step prediction to multi-step prediction\n",
    "            y_pred[i] = y_pred_i[0, 0, 0]\n",
    "\n",
    "            # update moving window\n",
    "            X_window = np.concatenate([X_window[:, 1:, :], y_pred_i], axis=1)\n",
    "\n",
    "    # get de-normalized ground truth, model predictions\n",
    "    y_test = scaler.inverse_transform(y_test)\n",
    "    y_pred = scaler.inverse_transform(y_pred)\n",
    "\n",
    "    # report mean-squared error\n",
    "    print('mse: %g' % (sklearn.metrics.mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "    return y_pred, y_test\n",
    "\n",
    "def lstm_forecast(x, window_size=100, train_size=0.8, n_steps=1):\n",
    "    # normalize input data to the range [-1, 1]\n",
    "    scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "    x = scaler.fit_transform(x)\n",
    "    x = pd.DataFrame(x)\n",
    "\n",
    "    # augment input data with time-shifted copies\n",
    "    x_orig = x.copy()\n",
    "\n",
    "    for i in range(window_size):\n",
    "        x = pd.concat([x, x_orig.shift(-(i+1))], axis=1)\n",
    "\n",
    "    # remove rows with missing data\n",
    "    x.dropna(axis=0, inplace=True)\n",
    "\n",
    "    # convert input data to numpy array\n",
    "    x = x.values\n",
    "    x = x.reshape(x.shape[0], x.shape[1], 1)\n",
    "\n",
    "    # create train/test sets\n",
    "    split_index = int(x.shape[0] * train_size)\n",
    "\n",
    "    train = x[:split_index, :]\n",
    "    test = x[split_index:, :]\n",
    "\n",
    "    train = sklearn.utils.shuffle(train)\n",
    "\n",
    "    X_train = train[:, :-1]\n",
    "    y_train = train[:, -1]\n",
    "    X_test = test[:, :-1]\n",
    "    y_test = test[:, -1]\n",
    "\n",
    "    # create lstm model\n",
    "    model = lstm_create(window_size)\n",
    "\n",
    "    # print model summary\n",
    "    # model.summary()\n",
    "\n",
    "    # train lstm model\n",
    "    lstm_train(model, X_train, y_train)\n",
    "\n",
    "    # compute forecast\n",
    "    y_pred, y_test = lstm_predict(model, X_test, y_test, scaler, n_steps)\n",
    "\n",
    "    return y_pred, y_test\n",
    "\n",
    "def lstm_plot(y_test, y_pred, channel_name):\n",
    "    s_rate = 256\n",
    "    t = np.arange(len(y_test)) / s_rate\n",
    "\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.plot(t, y_test, label='Actual')\n",
    "    plt.plot(t, y_pred, label='Predicted')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time (t) [s]')\n",
    "    plt.ylabel('Voltage (V) [V]')\n",
    "    plt.title('EEG Forecast for %s' % (channel_name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get channel csv filenames\n",
    "csv_files = glob.glob('seizure.*.csv')\n",
    "\n",
    "# initialize output arrays\n",
    "channels = []\n",
    "y_pred = []\n",
    "y_test = []\n",
    "\n",
    "# perform time series forecast for each channel\n",
    "for filename in csv_files:\n",
    "    # get channel name\n",
    "    channel_name = filename.split('.')[-2]\n",
    "\n",
    "    print(channel_name)\n",
    "\n",
    "    # read channel dataframe\n",
    "    df_channel = pd.read_csv(filename)\n",
    "\n",
    "    # perform forecasting for a random sample\n",
    "    column = random.choice(df_channel.columns)\n",
    "\n",
    "    # extract channel k, sample i\n",
    "    x_ki = pd.DataFrame(df_channel[column])\n",
    "\n",
    "    # perform time series forecasting\n",
    "    y_pred_ki, y_test_ki = lstm_forecast(x_ki)\n",
    "\n",
    "    # append results\n",
    "    channels.append(channel_name)\n",
    "    y_pred.append(y_pred_ki)\n",
    "    y_test.append(y_test_ki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot results\n",
    "for k in range(len(y_pred)):\n",
    "    lstm_plot(y_test[k], y_pred[k], channels[k])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (mlbd)",
   "language": "python",
   "name": "mlbd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
