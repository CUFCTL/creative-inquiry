{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of Normalization on Classification Potential\n",
    "\n",
    "Authors: Ben Shealy, Cole Younginer\n",
    "\n",
    "In this notebook, we'll explore how the __normalization__ of your dataset affects __classification potential__ (the ability of a classifier to distinguish between the different classes in the data). For background information on the data we'll be working with, refer to the Tumor Classification notebook.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "In this notebook we're going to work with RNA expression data derived from kidney tumor samples. This data is taken \n",
    "from __The Cancer Genome Atles (TCGA)__ project, which contains RNA sequences for a wide array of cancers. Our dataset contains samples from five types of cancer. This dataset is available on our [Box folder](https://clemson.app.box.com/folder/11145145746)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import sklearn.neural_network\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "The dataset consists of two files:\n",
    "\n",
    "- `tcga-5.fpkm.txt`: expression matrix with genes as rows and samples as columns\n",
    "- `tcga-5.labels.txt`: label file containing tumor type label for each sample\n",
    "\n",
    "We can load each of these files easily as pandas dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe\n",
    "X = pd.read_csv(\"tcga-5.fpkm.txt\", index_col=0, sep=\"\\t\")\n",
    "y = pd.read_csv(\"tcga-5.labels.txt\", sep=\"\\t\", header=None)\n",
    "\n",
    "# transpose data, fill missing values\n",
    "X = X.T\n",
    "X = X.fillna(X.min().min())\n",
    "\n",
    "# select a subset of genes\n",
    "n_genes = 1000\n",
    "\n",
    "genes = np.random.choice(len(X.columns), n_genes, replace=False)\n",
    "X = X.iloc[:, genes]\n",
    "\n",
    "# convert labels to numerical encoding\n",
    "le = sklearn.preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "classes = le.classes_\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting a Classifier\n",
    "\n",
    "An important aspect of this experiment is the classifier that we use. We could experiment with a variety of classifiers, but since our focus here is the effect of normalization, we'll stick to one classifier for now. Let's use a basic three-layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(clf, X, y, classes):\n",
    "    # perform train/test split\n",
    "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "    # train classifer\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # compute predicted labels for test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # compute accuracy score\n",
    "    score = sklearn.metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # create a confusion matrix from the class predictions\n",
    "    cnf_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    sns.heatmap(cnf_matrix, annot=True, fmt=\"d\", cbar=False, square=True, xticklabels=classes, yticklabels=classes)\n",
    "    plt.ylabel(\"Expected\")\n",
    "    plt.xlabel(\"Measured\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scalers = [\n",
    "    (\"passthrough\", None),\n",
    "    (\"log2\", sklearn.preprocessing.FunctionTransformer(func=np.log2)),\n",
    "    (\"maxabs\", sklearn.preprocessing.MaxAbsScaler()),\n",
    "    (\"minmax\", sklearn.preprocessing.MinMaxScaler()),\n",
    "    (\"quantile\", sklearn.preprocessing.QuantileTransformer(output_distribution=\"normal\")),\n",
    "    (\"robust\", sklearn.preprocessing.RobustScaler()),\n",
    "    (\"standard\", sklearn.preprocessing.StandardScaler())\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    (\"mlp\", sklearn.neural_network.MLPClassifier(solver=\"adam\", alpha=1e-4, hidden_layer_sizes=(256)))\n",
    "]\n",
    "\n",
    "for scaler_name, scaler in scalers:\n",
    "    for clf_name, clf in classifiers:\n",
    "        pipeline = sklearn.pipeline.Pipeline([\n",
    "            (scaler_name, scaler),\n",
    "            (clf_name, clf)\n",
    "        ])\n",
    "\n",
    "        score = evaluate(clf, X, y, classes)\n",
    "        print(\"%s, %s: %0.3f\" % (scaler_name, clf_name, score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (mlbd)",
   "language": "python",
   "name": "mlbd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
